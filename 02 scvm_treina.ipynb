{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Escolha do modelo\n",
    "- ## Treinamento do modelo\n",
    "- ## Teste do modelo\n",
    "- ## Armazenamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximiza nro de linhas e colunas para exibição\n",
    "# inibe mensagens de warning\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None) # permite a máxima visualização das linhas em um display\n",
    "pd.set_option('display.max_columns', None) # permite a máxima visualização das colunas em um display\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # inibe a exibição de avisos de warning\n",
    "warnings.filterwarnings('ignore') # inibe a exibição de avisos de warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelos de aprendizado por reforço\n",
    "import keras \n",
    "from keras import layers # são os blocos básicos de construção de redes neurais no Keras, consistindo em funções de computação que recebem tensores de entrada e produzem tensores de saída.\n",
    "from keras import models # Keras oferece uma série de aplicações com modelos de aprendizado profundo e pesos pré-treinados que podem ser utilizadas para predições e extração de características.\n",
    "from keras import optimizers # permite que você acesse diferentes algoritmos de otimização para treinar seu modelo.\n",
    "from keras import backend as K # Através do tf.keras.backend, você pode acessar várias funções utilitárias, como determinar o backend atual e realizar operações em tensores.\n",
    "from collections import namedtuple # proporcionam uma forma leve de criar estruturas de dados simples, que se comportam como classes, mas são imutáveis e com sintaxe mais limpa.\n",
    "from collections import deque # A partir do módulo 'collections', você pode utilizar o deque para gerenciar uma coleção de dados com operações rápidas de adição e remoção.\n",
    "\n",
    "from keras.models import Sequential # O modelo Sequential é apropriado para uma pilha simples de camadas, onde cada camada tem exatamente um tensor de entrada e um tensor de saída.\n",
    "\n",
    "from keras.models import load_model # função load_model do Keras é utilizada para carregar modelos que foram salvos anteriormente usando model.save(). Isso permite que você recupere modelos treinados do armazenamento para uso posterior.\n",
    "from keras.layers import Dense # A camada Dense em Keras é uma camada de rede neural densamente conectada que realiza a operação: saída = ativação(dot(entrada, peso) + viés).\n",
    "from keras.optimizers import Adam # Adam é um método que se adapta à taxa de aprendizado, sendo amplamente utilizado para otimização em modelos de aprendizado de máquina. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparação e visualização de dados\n",
    "import numpy as np # computação de matrizes\n",
    "from matplotlib import pyplot as plt # plotagem gráfica\n",
    "import seaborn as sns # plotagem gráfica\n",
    "import matplotlib.ticker as ticker  # plotagem gráfica\n",
    "\n",
    "import datetime # O módulo datetime fornece classes para manipular datas e horários,\n",
    "import math # O módulo math em Python fornece acesso a funções matemáticas definidas pelo padrão C, como trigonometria e manipulação de números.\n",
    "import random # é utilizado para gerar números pseudoaleatórios e realizar diversas operações de aleatorização, como escolher elementos aleatórios de uma lista.\n",
    "from numpy.random import choice # A função choice() pode ser usada para retornar um único valor aleatório de um array em Python, permitindo a manipulação de amostras aleatórias de forma fácil.\n",
    "from collections import deque # O deque, ou fila de duas extremidades, é uma estrutura de dados que permite a inserção e remoção eficiente de elementos em ambas as extremidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando arquivo tratado em dataframe\n",
    "df_soy = pd.read_csv('df_treated/df_soy_treated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9276, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conferindo a dimensão do dataframe, linhas e colunas\n",
    "df_soy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenando pelo index o dataframe carregado\n",
    "df_soy = df_soy.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open_Interest</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>567.0</td>\n",
       "      <td>570.00</td>\n",
       "      <td>566.0</td>\n",
       "      <td>569.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-01-05</td>\n",
       "      <td>570.5</td>\n",
       "      <td>574.00</td>\n",
       "      <td>570.5</td>\n",
       "      <td>573.50</td>\n",
       "      <td>12.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-01-06</td>\n",
       "      <td>574.0</td>\n",
       "      <td>579.00</td>\n",
       "      <td>574.0</td>\n",
       "      <td>577.00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-01-07</td>\n",
       "      <td>574.5</td>\n",
       "      <td>575.75</td>\n",
       "      <td>574.0</td>\n",
       "      <td>574.25</td>\n",
       "      <td>69.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-01-08</td>\n",
       "      <td>579.0</td>\n",
       "      <td>581.00</td>\n",
       "      <td>579.0</td>\n",
       "      <td>580.50</td>\n",
       "      <td>61.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Open    High    Low   Close  Volume  Open_Interest  Year\n",
       "0  1999-01-04  567.0  570.00  566.0  569.00    14.0          312.0  1999\n",
       "1  1999-01-05  570.5  574.00  570.5  573.50    12.0          323.0  1999\n",
       "2  1999-01-06  574.0  579.00  574.0  577.00    43.0          329.0  1999\n",
       "3  1999-01-07  574.5  575.75  574.0  574.25    69.0          348.0  1999\n",
       "4  1999-01-08  579.0  581.00  579.0  580.50    61.0          345.0  1999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconhecendo as primeiras linhas do dataframe\n",
    "df_soy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9276 entries, 0 to 9275\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           9276 non-null   object \n",
      " 1   Open           9276 non-null   float64\n",
      " 2   High           9276 non-null   float64\n",
      " 3   Low            9276 non-null   float64\n",
      " 4   Close          9276 non-null   float64\n",
      " 5   Volume         9276 non-null   float64\n",
      " 6   Open_Interest  9276 non-null   float64\n",
      " 7   Year           9276 non-null   int64  \n",
      "dtypes: float64(6), int64(1), object(1)\n",
      "memory usage: 579.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# verificando a posição de cada variável\n",
    "df_soy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year\n",
       "2010    510\n",
       "2019    510\n",
       "2009    502\n",
       "2016    502\n",
       "2015    502\n",
       "2013    502\n",
       "2012    502\n",
       "2014    501\n",
       "2018    500\n",
       "2017    499\n",
       "2020    490\n",
       "2011    469\n",
       "2007    406\n",
       "2008    354\n",
       "2005    323\n",
       "2006    320\n",
       "2002    308\n",
       "2004    280\n",
       "2003    270\n",
       "2000    267\n",
       "2001    259\n",
       "1999    251\n",
       "2021    249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificando quantas cotações existem por ano\n",
    "df_soy.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando o tipo de Date para datetime\n",
    "df_soy['Date'] = df_soy['Date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando somente o ano de 2021 - último ano dos dados de cotações\n",
    "#df_soy = df_soy.loc[df_soy['Date'].dt.year == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando uma lista dos valores das colunas que serão utilizadas para o treinamento e teste\n",
    "X=list(df_soy['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando somente algumas linhas para fazer uma prévia rápida e conferir o código\n",
    "X= X[9256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando as variáveis de treino(80%) e de teste(20%)\n",
    "X=[float(x) for x in X] \n",
    "validation_size = 0.2 \n",
    "train_size = int(len(X) * (1-validation_size)) \n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# verificando quantas ocorrências de treino e de teste foram geradas\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1267.25,\n",
       " 1261.5,\n",
       " 1250.25,\n",
       " 1261.0,\n",
       " 1264.5,\n",
       " 1267.75,\n",
       " 1244.0,\n",
       " 1259.5,\n",
       " 1262.5,\n",
       " 1277.25]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificando as primeiras ocorrências da lista de valores de treino\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1359.25, 1356.5, 1327.75, 1328.75]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificando as primeiras ocorrências da lista de valores de teste\n",
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declara a classe Agent\n",
    "# instancia o modelo sequential\n",
    "# treina e ajusta o modelo\n",
    "\n",
    "class Agent: \n",
    "    def __init__(self, state_size, is_eval=False, model_name=''): \n",
    "        self.state_size = state_size # dias anteriores normalizados \n",
    "        self.action_size = 3 # hold, compra, venda \n",
    "        self.memory = deque(maxlen=1000) \n",
    "        self.inventory = [] \n",
    "        self.model_name = model_name \n",
    "        self.is_eval = is_eval \n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0 \n",
    "        self.epsilon_min = 0.01 \n",
    "        self.epsilon_decay = 0.995 \n",
    "        self.model = load_model('models/' + model_name) if is_eval else self._model()\n",
    "\n",
    "    def _model(self): \n",
    "        model = Sequential() \n",
    "        model.add(Dense(units=64, input_dim=self.state_size, activation='relu')) \n",
    "        model.add(Dense(units=32, activation='relu')) \n",
    "        model.add(Dense(units=8, activation='relu')) \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001)) \n",
    "        return model     \n",
    "\n",
    "    def act(self, state): \n",
    "        if not self.is_eval and random.random() <= self.epsilon: \n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        options = self.model.predict(state) \n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = [] \n",
    "        l = len(self.memory) \n",
    "\n",
    "        #1: preparar a replay memory \n",
    "        for i in range(l - batch_size + 1, l): \n",
    "            mini_batch.append(self.memory[i]) \n",
    "\n",
    "        #2: Fazer loop em todo o lote de replay. \n",
    "        for state, action, reward, next_state, done in mini_batch: \n",
    "            target = reward # recompensa ou Q no tempo t \n",
    "\n",
    "            #3: atualizar o alvo para tabela Q. equação de tabela\n",
    "            if not done: \n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) #set_trace() \n",
    "            \n",
    "            #4: Valor Q do estado atual a partir da tabela \n",
    "            target_f = self.model.predict(state) \n",
    "            \n",
    "            #5: Atualizar a tabela Q de saída para a ação dada na tabela \n",
    "            target_f[0][action] = target \n",
    "            \n",
    "            #6. Treinar e ajustar o modelo \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "            #7. Implementar algoritmo epsilon greedy \n",
    "            if self.epsilon > self.epsilon_min: \n",
    "                self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As recompensas são computadas.\n",
    "- Os pesos do modelo Q-learning baseado em aprendizado profundo são atualizados iterativamente ao longo de diversos episódios.\n",
    "- O lucro e a perda de cada título são somados para determinar se um lucro geral ocorreu. \n",
    "- O objetivo é maximizar o lucro total.\n",
    "\n",
    "- A função MODEL é um modelo de aprendizado profundo que conecta os estados com as ações.\n",
    "- O modelo espera linhas de dados com números de variáveis iguais ao tamanho do estado, que chega como uma entrada.\n",
    "- A primeira, a segunda e a terceira camadas ocultas têm 64, 32 e 8 nós, respectivamente, e todas essas camadas usam a função de ativação ReLU. \n",
    "- A camada de saída tem o número de nós igual ao tamanho da ação (três), e o nó usa uma função de ativação linear.\n",
    "\n",
    "- A função ACT retorna uma ação dado um estado. Ela usa a função model e retorna uma ação de compra, venda ou hold.\n",
    "\n",
    "- A função EXPREPLAY é a função-chave, na qual a rede neural é treinada com base na experiência observada. Essa função implementa o mecanismo replay de experiência. O replay de experiência armazena um histórico de estado, ação, recompensa e próximas transições de estado que são experienciadas pelo agente.\n",
    "- A abordagem epsilon greedy implementada nessa função impede o sobreajuste.\n",
    "- DESCRIÇÃO DOS PASSOS: Preparar o replay buffer memory, que é o conjunto de observações usado para o treinamento. Novas experiências são acrescentadas ao replay buffer memory usando um loop for. Fazer o loop em todas as observações de estado, ação, recompensa e próximas transições de estado no minilote. A variável-alvo para a tabela Q é atualizada com base na equação de Bellman. A atualização acontece se o estado atual for o estado terminal ou o fim do episódio. Isso é representado pela variável done e é definido ainda na função de treinamento. Caso não se classificado como done, o alvo é ajustado para recompensa. Prever o valor Q do próximo estágio usando um modelo de aprendizado profundo. O valor Q desse estado para a ação no replay buffer atual é ajustado para alvo. Os pesos do modelo de aprendizado profundo são atualizados usando a função model.fit. A abordagem epsilon greedy é implementada. Lembre-se de que essa abordagem seleciona uma ação de forma aleatória com uma probabilidade de ε ou a melhor ação, de acordo com a função de valor Q, com a probabilidade de 1–ε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula um valor entre 0 e 1 resultado da função sigmoide, usando um valor x passado.\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A função sigmoide é uma das funções de ativação mais conhecidas e se destaca por produzir resultados entre 0 e 1, concentrando valores próximos a essas extremidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera o estado considerando os dados da ação, o tempo t (dia da previsão) \n",
    "# e a janela n (número de dias para voltar no tempo).\n",
    "def getState(data, t, n): \n",
    "    d = t - n + 1 \n",
    "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1]\n",
    "\n",
    "    res = [] \n",
    "    for i in range(n - 1): \n",
    "        res.append(sigmoid(block[i + 1] - block[i])) \n",
    "        \n",
    "    return np.array([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna a plotagem do preço do mercado juntamente com indicadores para as títulos de compra e de venda.\n",
    "def plot_behavior(data_input, states_buy, states_sell, profit):\n",
    "    fig = plt.figure(figsize = (15, 5)) \n",
    "    plt.plot(data_input, color='r', lw=2.) \n",
    "    plt.plot(data_input, '^', markersize=10, color='m', label='Buying signal', markevery=states_buy) \n",
    "    plt.plot(data_input, 'v', markersize=10, color='k', label='Selling signal', markevery = states_sell) \n",
    "    plt.title('Total gains: %f'%(profit)) \n",
    "    plt.legend() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recebe um valor float e retorna formatado com 2 decimais\n",
    "def formatPrice(vr):\n",
    "    return (\"%.2f\" % vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoca o treinamento de alguns modelos cuja quantidade é do episode_count considerando iniciar em 0\n",
    "# armazena os modelos treinados na pasta models\n",
    "# para cada action retornado do agent exibe o resultado como uma ação de buy(comprar), sell(vender) ou\n",
    "# hold(manter)\n",
    "window_size = 1\n",
    "agent = Agent(window_size)\n",
    "data = X_train # ....................\n",
    "l = len(data) - 1 \n",
    "batch_size = 10 \n",
    "states_sell = [] \n",
    "states_buy = [] \n",
    "episode_count = 3 \n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"INÍCIO DO TREINO - Data e hora atuais: \", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "for e in range(episode_count + 1): \n",
    "    print('Episode ' + str(e) + '/' + str(episode_count)) \n",
    "    \n",
    "    # 1-obter estado \n",
    "    state = getState(data, 0, window_size + 1) \n",
    "    total_profit = 0 \n",
    "    agent.inventory = []\n",
    "\n",
    "    for t in range(l): \n",
    "        # 2-aplicar a melhor ação \n",
    "        action = agent.act(state) \n",
    "\n",
    "        # sit \n",
    "        next_state = getState(data, t + 1, window_size + 1) \n",
    "        reward = 0 \n",
    "        bought_price = 0 # ....................\n",
    "        if action == 1: # compra \n",
    "            states_buy.append(t) \n",
    "            print('1 - Buy (comprar): ' + formatPrice(data[t]))\n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: # venda\n",
    "            bought_price = agent.inventory.pop(0) \n",
    "        \n",
    "            #3: Obter Recompensa \n",
    "            reward = max(data[t] - bought_price, 0) \n",
    "            total_profit += data[t] - bought_price \n",
    "            states_sell.append(t) \n",
    "            print('2 - Sell (vender): ' + formatPrice(data[t]) + ' | Profit (lucro): ' + formatPrice(data[t] - bought_price))\n",
    "\n",
    "        else:\n",
    "            print('3 - Hold (manter): ' + formatPrice(data[t]))    \n",
    "\n",
    "        done = True if t == l - 1 else False \n",
    "        \n",
    "        # 4: Obter próximo estado a ser usado na equação de Bellman \n",
    "        next_state = getState(data, t + 1, window_size + 1) \n",
    "        \n",
    "        # 5: Acrescentar à memória \n",
    "        agent.memory.append((state, action, reward, next_state, done)) \n",
    "        state = next_state \n",
    "        if done: \n",
    "            print('--------------------------------')\n",
    "            print('Total do Lucro: ' + formatPrice(total_profit)) \n",
    "            print('--------------------------------') \n",
    "\n",
    "        print('e: ' + str(e) + ' | l: ' + str(l) + ' | t: ' + str(t) + ' | action: ' + str(action) + \\\n",
    "              ' | bought_price: ' + formatPrice(bought_price)) \n",
    "        print('lent(agent.memory): ' + str(len(agent.memory)) + ' | batch_size: ' + str(batch_size))\n",
    "\n",
    "        # 6: Executar função replay buffer \n",
    "        if len(agent.memory) > batch_size: \n",
    "            agent.expReplay(batch_size) \n",
    "\n",
    "        if e % 10 == 0: \n",
    "            agent.model.save('models/model_ep' + str(e) + '.keras')\n",
    "\n",
    "    agent.model.save('models/model_ep' + str(e) + '.keras')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"FIM DO TREINO - Data e hora atuais: \", now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# invoca o teste dos modelos utilizando os valores da lista de teste X_test\n",
    "# o agente já está definido no conjunto de teste precedente. \n",
    "# para cada action retornado do agent exibe o resultado como uma ação de buy(comprar), sell(vender) ou\n",
    "# hold(manter)\n",
    "def testa_modelos(X_test, window_size, model_name):\n",
    "    test_data = X_test \n",
    "    l_test = len(test_data) - 1 \n",
    "    state = getState(test_data, 0, window_size + 1) \n",
    "    total_profit = 0 \n",
    "    is_eval = True \n",
    "    done = False \n",
    "    states_sell_test = [] \n",
    "    states_buy_test = [] \n",
    "    model_name = model_name #'model_ep3.keras'\n",
    "    agent = Agent(window_size, is_eval, model_name) \n",
    "    state = getState(data, 0, window_size + 1) \n",
    "    total_profit = 0 \n",
    "    agent.inventory = [] \n",
    "\n",
    "    for t in range(l_test): \n",
    "        action = agent.act(state) \n",
    "        next_state = getState(test_data, t + 1, window_size + 1) \n",
    "        reward = 0 \n",
    "\n",
    "        if action == 1:\n",
    "            agent.inventory.append(test_data[t]) \n",
    "            print('Buy (comprar): ' + formatPrice(test_data[t])) \n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: \n",
    "            bought_price = agent.inventory.pop(0) \n",
    "            reward = max(test_data[t] - bought_price, 0) \n",
    "            total_profit += test_data[t] - bought_price\n",
    "            print('Sell (vender): ' + formatPrice(test_data[t]) + ' | profit (lucro): ' + formatPrice(test_data[t] - bought_price)) \n",
    "        \n",
    "        else:\n",
    "            print('3 - Hold (manter): ' + formatPrice(test_data[t]))         \n",
    "\n",
    "        if t == l_test - 1: \n",
    "            done = True \n",
    "            agent.memory.append((state, action, reward, next_state, done)) \n",
    "            state = next_state \n",
    "\n",
    "    if done: \n",
    "        print('------------------------------------------')\n",
    "        print('Total do Lucro: ' + formatPrice(total_profit)) \n",
    "        print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ep0.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "3 - Hold: 1359.25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "3 - Hold: 1356.50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "3 - Hold: 1327.75\n",
      "------------------------------------------\n",
      "Total Profit: 0.00\n",
      "------------------------------------------\n",
      "model_ep1.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "3 - Hold: 1359.25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "3 - Hold: 1356.50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "3 - Hold: 1327.75\n",
      "------------------------------------------\n",
      "Total Profit: 0.00\n",
      "------------------------------------------\n",
      "model_ep2.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "3 - Hold: 1359.25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "3 - Hold: 1356.50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "3 - Hold: 1327.75\n",
      "------------------------------------------\n",
      "Total Profit: 0.00\n",
      "------------------------------------------\n",
      "model_ep3.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Buy: 1359.25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Buy: 1356.50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Buy: 1327.75\n",
      "------------------------------------------\n",
      "Total Profit: 0.00\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# invoca a função testa_modelos para o teste de predição em todos os modelos treinados\n",
    "now = datetime.datetime.now()\n",
    "print(\"INÍCIO DO TESTE - Data e hora atuais: \", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "for i in (0, 1, 2, 3):\n",
    "    model_name = 'model_ep' + str(i) + '.keras'\n",
    "\n",
    "    print(model_name)\n",
    "    testa_modelos(X_test, window_size, model_name)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"FIM DO TESTE - Data e hora atuais: \", now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
