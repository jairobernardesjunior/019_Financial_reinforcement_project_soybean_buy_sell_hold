{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Escolha do modelo\n",
    "- ## Treinamento do modelo\n",
    "- ## Teste do modelo\n",
    "- ## Armazenamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximiza nro de linhas e colunas para exibição\n",
    "# inibe mensagens de warning\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None) # permite a máxima visualização das linhas em um display\n",
    "pd.set_option('display.max_columns', None) # permite a máxima visualização das colunas em um display\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # inibe a exibição de avisos de warning\n",
    "warnings.filterwarnings('ignore') # inibe a exibição de avisos de warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelos de aprendizado por reforço\n",
    "import keras \n",
    "from keras import layers, models, optimizers \n",
    "from keras import backend as K \n",
    "from collections import namedtuple, deque \n",
    "from keras.models import Sequential \n",
    "from keras.models import load_model \n",
    "from keras.layers import Dense \n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparação e visualização de dados\n",
    "import numpy as np # computação de matrizes\n",
    "from matplotlib import pyplot as plt # plotagem gráfica\n",
    "import seaborn as sns # plotagem gráfica\n",
    "import matplotlib.ticker as ticker  # plotagem gráfica\n",
    "\n",
    "import datetime \n",
    "import math\n",
    "import random\n",
    "from numpy.random import choice \n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando arquivo tratado em dataframe\n",
    "df_soy = pd.read_csv('df_treated/df_soy_treated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9276, 8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conferindo a dimensão do dataframe, linhas e colunas\n",
    "df_soy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenando pelo index o dataframe carregado\n",
    "df_soy = df_soy.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open_Interest</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>567.0</td>\n",
       "      <td>570.00</td>\n",
       "      <td>566.0</td>\n",
       "      <td>569.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-01-05</td>\n",
       "      <td>570.5</td>\n",
       "      <td>574.00</td>\n",
       "      <td>570.5</td>\n",
       "      <td>573.50</td>\n",
       "      <td>12.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-01-06</td>\n",
       "      <td>574.0</td>\n",
       "      <td>579.00</td>\n",
       "      <td>574.0</td>\n",
       "      <td>577.00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-01-07</td>\n",
       "      <td>574.5</td>\n",
       "      <td>575.75</td>\n",
       "      <td>574.0</td>\n",
       "      <td>574.25</td>\n",
       "      <td>69.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-01-08</td>\n",
       "      <td>579.0</td>\n",
       "      <td>581.00</td>\n",
       "      <td>579.0</td>\n",
       "      <td>580.50</td>\n",
       "      <td>61.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Open    High    Low   Close  Volume  Open_Interest  Year\n",
       "0  1999-01-04  567.0  570.00  566.0  569.00    14.0          312.0  1999\n",
       "1  1999-01-05  570.5  574.00  570.5  573.50    12.0          323.0  1999\n",
       "2  1999-01-06  574.0  579.00  574.0  577.00    43.0          329.0  1999\n",
       "3  1999-01-07  574.5  575.75  574.0  574.25    69.0          348.0  1999\n",
       "4  1999-01-08  579.0  581.00  579.0  580.50    61.0          345.0  1999"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconhecendo as primeiras linhas do dataframe\n",
    "df_soy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9276 entries, 0 to 9275\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           9276 non-null   object \n",
      " 1   Open           9276 non-null   float64\n",
      " 2   High           9276 non-null   float64\n",
      " 3   Low            9276 non-null   float64\n",
      " 4   Close          9276 non-null   float64\n",
      " 5   Volume         9276 non-null   float64\n",
      " 6   Open_Interest  9276 non-null   float64\n",
      " 7   Year           9276 non-null   int64  \n",
      "dtypes: float64(6), int64(1), object(1)\n",
      "memory usage: 579.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# verificando a posição de cada variável\n",
    "df_soy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soy['Date'] = df_soy['Date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=list(df_soy['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[float(x) for x in X] \n",
    "validation_size = 0.2 \n",
    "train_size = int(len(X) * (1-validation_size)) \n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7420\n",
      "1856\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent: \n",
    "    def __init__(self, state_size, is_eval=False, model_name=''): \n",
    "        self.state_size = state_size # dias anteriores normalizados \n",
    "        self.action_size = 3 # hold, compra, venda \n",
    "        self.memory = deque(maxlen=1000) \n",
    "        self.inventory = [] \n",
    "        self.model_name = model_name \n",
    "        self.is_eval = is_eval \n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0 \n",
    "        self.epsilon_min = 0.01 \n",
    "        self.epsilon_decay = 0.995 \n",
    "        self.model = load_model('models/' + model_name) if is_eval else self._model()\n",
    "\n",
    "    def _model(self): \n",
    "        model = Sequential() \n",
    "        model.add(Dense(units=64, input_dim=self.state_size, activation='relu')) \n",
    "        model.add(Dense(units=32, activation='relu')) \n",
    "        model.add(Dense(units=8, activation='relu')) \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001)) \n",
    "        return model     \n",
    "\n",
    "    def act(self, state): \n",
    "        if not self.is_eval and random.random() <= self.epsilon: \n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        options = self.model.predict(state) \n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = [] \n",
    "        l = len(self.memory) \n",
    "\n",
    "        #1: preparar a replay memory \n",
    "        for i in range(l - batch_size + 1, l): \n",
    "            mini_batch.append(self.memory[i]) \n",
    "\n",
    "        #2: Fazer loop em todo o lote de replay. \n",
    "        for state, action, reward, next_state, done in mini_batch: \n",
    "            target = reward # recompensa ou Q no tempo t \n",
    "                \n",
    "            #3: atualizar o alvo para tabela Q. equação de tabela\n",
    "            if not done: \n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) #set_trace() \n",
    "            \n",
    "            #4: Valor Q do estado atual a partir da tabela \n",
    "            target_f = self.model.predict(state) \n",
    "            \n",
    "            #5: Atualizar a tabela Q de saída para a ação dada na tabela \n",
    "            target_f[0][action] = target \n",
    "            \n",
    "            #6. Treinar e ajustar o modelo \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "            #7. Implementar algoritmo epsilon greedy \n",
    "            if self.epsilon > self.epsilon_min: \n",
    "                self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState(data, t, n): \n",
    "    d = t - n + 1 \n",
    "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1]\n",
    "\n",
    "    res = [] \n",
    "    for i in range(n - 1): \n",
    "        res.append(sigmoid(block[i + 1] - block[i])) \n",
    "        \n",
    "    return np.array([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_behavior(data_input, states_buy, states_sell, profit):\n",
    "    fig = plt.figure(figsize = (15, 5)) \n",
    "    plt.plot(data_input, color='r', lw=2.) \n",
    "    plt.plot(data_input, '^', markersize=10, color='m', label='Buying signal', markevery=states_buy) \n",
    "    plt.plot(data_input, 'v', markersize=10, color='k', label='Selling signal', markevery = states_sell) \n",
    "    plt.title('Total gains: %f'%(profit)) \n",
    "    plt.legend() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.001}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \n",
      "Cell \u001b[1;32mIn[81], line 13\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[1;34m(self, state_size, is_eval, model_name)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m \n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model_name) \u001b[38;5;28;01mif\u001b[39;00m is_eval \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[81], line 21\u001b[0m, in \u001b[0;36mAgent._model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m) \n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\KR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m ):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[1;32mc:\\Users\\KR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:21\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32mc:\\Users\\KR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:90\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.001}"
     ]
    }
   ],
   "source": [
    "window_size = 1 \n",
    "agent = Agent(window_size)\n",
    "l = len(data) - 1 \n",
    "batch_size = 10 \n",
    "states_sell = [] \n",
    "states_buy = [] \n",
    "episode_count = 3 \n",
    "\n",
    "for e in range(episode_count + 1): \n",
    "    print('Episode ' + str(e) + '/' + str(episode_count)) \n",
    "    \n",
    "    # 1-obter estado \n",
    "    state = getState(data, 0, window_size + 1) \n",
    "    total_profit = 0 \n",
    "    agent.inventory = []\n",
    "\n",
    "    for t in range(l): \n",
    "        # 2-aplicar a melhor ação \n",
    "        action = agent.act(state) \n",
    "        \n",
    "        # sit \n",
    "        next_state = getState(data, t + 1, window_size + 1) \n",
    "        reward = 0 \n",
    "        if action == 1: # compra \n",
    "            states_buy.append(t) \n",
    "            print('Buy: ' + formatPrice(data[t])) \n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: # venda\n",
    "            bought_price = agent.inventory.pop(0) \n",
    "            \n",
    "        #3: Obter Recompensa \n",
    "        reward = max(data[t] - bought_price, 0) \n",
    "        total_profit += data[t] - bought_price \n",
    "        states_sell.append(t) \n",
    "        print('Sell: ' + formatPrice(data[t]) + ' | Profit: ' + formatPrice(data[t] - bought_price))\n",
    "\n",
    "        done = True if t == l - 1 else False \n",
    "        \n",
    "        # 4: Obter próximo estado a ser usado na equação de Bellman \n",
    "        next_state = getState(data, t + 1, window_size + 1) \n",
    "        \n",
    "        # 5: Acrescentar à memória \n",
    "        agent.memory.append((state, action, reward, next_state, done)) \n",
    "        state = next_state \n",
    "        if done: \n",
    "            print('--------------------------------')\n",
    "            print('Total Profit: ' + formatPrice(total_profit)) \n",
    "            print('--------------------------------') \n",
    "    \n",
    "        # 6: Executar função replay buffer \n",
    "        if len(agent.memory) > batch_size: \n",
    "            agent.expReplay(batch_size) \n",
    "\n",
    "        if e % 10 == 0: \n",
    "            agent.model.save('models/model_ep' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O agente já está definido no conjunto de teste precedente. \n",
    "test_data = X_test \n",
    "l_test = len(test_data) - 1 \n",
    "state = getState(test_data, 0, window_size + 1) \n",
    "total_profit = 0 \n",
    "is_eval = True \n",
    "done = False \n",
    "states_sell_test = [] \n",
    "states_buy_test = [] \n",
    "model_name = 'model_ep10'\n",
    "agent = Agent(window_size, is_eval, model_name) \n",
    "state = getState(data, 0, window_size + 1) \n",
    "total_profit = 0 \n",
    "agent.inventory = [] \n",
    "\n",
    "for t in range(l_test): \n",
    "    action = agent.act(state) \n",
    "    next_state = getState(test_data, t + 1, window_size + 1) \n",
    "    reward = 0 \n",
    "\n",
    "    if action == 1:\n",
    "        agent.inventory.append(test_data[t]) \n",
    "        print('Buy: ' + formatPrice(test_data[t])) \n",
    "\n",
    "    elif action == 2 and len(agent.inventory) > 0: \n",
    "        bought_price = agent.inventory.pop(0) \n",
    "        reward = max(test_data[t] - bought_price, 0) \n",
    "        total_profit += test_data[t] - bought_price\n",
    "        print('Sell: ' + formatPrice(test_data[t]) + ' | profit: ' + formatPrice(test_data[t] - bought_price)) \n",
    "    \n",
    "    if t == l_test - 1: \n",
    "        done = True \n",
    "        agent.memory.append((state, action, reward, next_state, done)) \n",
    "        state = next_state \n",
    "\n",
    "if done: \n",
    "    print('------------------------------------------')\n",
    "    print('Total Profit: ' + formatPrice(total_profit)) \n",
    "    print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o modelo de geração de clusters em arquivo .pkl\n",
    "#import joblib\n",
    "#joblib.dump(k_means, \"model/kmeans_card_holder.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
